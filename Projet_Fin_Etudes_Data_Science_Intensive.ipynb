{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMJs7Hf5sQcszFo3QOezpaz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ihien/ChatBot/blob/main/Projet_Fin_Etudes_Data_Science_Intensive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div align=\"center\">\n",
        "  <img src=\"https://dit.sn/wp-content/uploads/2023/10/Logo-1.png\" alt=\"Logo Université\" width=\"350\"/>\n",
        "  <h1></h1>\n",
        "  <h1>Dakar Institute of Technology</h1>\n",
        "  <h3>Data Science Department</h3>\n",
        "</div>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<div align=\"center\">\n",
        "  <h1>Projet de Fin d'Étude</h1>\n",
        "  <em>Pour l'obtention du :</em>\n",
        "  <br>Certificat Data Science Intensive<br>\n",
        "  <h4>Thème :</h4>\n",
        "  <h2>Mise en Place d'un ChatBot Basé sur LLM Pour Répondre aux Questions Usuelles des Clients</h2>\n",
        "</div>\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "<div align=\"center\">\n",
        "  <div style=\"display: flex; justify-content: space-around;\">\n",
        "    <div style=\"text-align: left;\">\n",
        "      <strong>Présenté par :</strong><br>\n",
        "      Isidore HIEN<br>\n",
        "      <em>Matricule : [Votre Matricule]</em>\n",
        "    </div>\n",
        "    <div style=\"text-align: left;\">\n",
        "      <strong>Sous la direction de :</strong><br>\n",
        "      Mouhamadou Naby DIA<br>\n",
        "      <em>NLP ENgineer</em>\n",
        "    </div>\n",
        "  </div>\n",
        "</div>\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "<div align=\"center\">\n",
        "  <p>Année Académique : 2025-2026</p>\n",
        "</div>"
      ],
      "metadata": {
        "id": "OOnLj9jVz5BZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Sommaire\n",
        "1.  [Introduction](#introduction)\n",
        "2.  [Activation du GPU](https://colab.research.google.com/drive/16TqNaEYSplpgJ6R1tiXSrlsxnx7X-X15#scrollTo=02JX67cZtVAQ&line=5&uniqifier=1)\n",
        "3.  [Modélisation et Entraînement](#modelisation)\n",
        "4.  [Résultats et Évaluation](#resultats)\n",
        "5.  [Conclusion](#conclusion)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "9xw1l-n97GkK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "Dans un contexte économique de plus en plus concurrentiel, la qualité et la réactivité du service client sont devenues des facteurs de différenciation majeurs pour les entreprises. Le centre d'appels, en tant que principal point de contact, se trouve au cœur de cette dynamique. Cependant, il est souvent confronté à des défis de taille : un volume d'appels élevé, des temps d'attente prolongés pour les clients, et une charge de travail répétitive pour les agents, qui doivent répondre continuellement aux mêmes questions fréquentes. Cette situation engendre non seulement une augmentation des coûts opérationnels, mais limite également la capacité des agents à se concentrer sur des problématiques complexes à plus forte valeur ajoutée.\n",
        "\n",
        "Face à cette problématique, les avancées récentes dans le domaine de l'intelligence artificielle, et plus particulièrement des grands modèles de langage (LLM), offrent des opportunités de transformation sans précédent. Ces technologies permettent de créer des agents conversationnels (chatbots) capables de comprendre et de traiter le langage naturel avec une fluidité et une pertinence remarquables.\n",
        "\n",
        "Ce projet vise à concevoir et mettre en œuvre un chatbot intelligent basé sur un LLM open source, destiné à automatiser la gestion des questions usuelles au sein d'un centre d'appels. En s'appuyant sur la technique de **Génération Augmentée par Récupération (RAG)**, notre solution ne se contentera pas de fournir des réponses génériques ; elle puisera ses informations directement dans une base de connaissance interne et sécurisée, garantissant ainsi des réponses précises, fiables et parfaitement adaptées au contexte de l'entreprise.\n",
        "\n",
        "Les principaux **objectifs** de ce projet sont les suivants :\n",
        "* **Développer** un agent conversationnel robuste en utilisant un LLM open source (`Mistral`) hébergé localement.\n",
        "* **Implémenter** un système de qualification systématique pour identifier chaque interlocuteur (nom, secteur d'activité, contact) avant de traiter sa demande.\n",
        "* **Connecter** le chatbot à une base de connaissance privée composée de documents d'entreprise (PDF, TXT, etc.).\n",
        "* **Assurer** la fiabilité des réponses en forçant le modèle à se baser exclusivement sur les documents fournis, minimisant ainsi les risques d'hallucination.\n",
        "* **Déployer** une démonstration fonctionnelle de l'application dotée d'une interface utilisateur interactive.\n",
        "\n",
        "Ce rapport détaillera l'ensemble de la méthodologie adoptée, de la configuration de l'environnement technique à la préparation de la base de connaissance, en passant par l'implémentation de la logique conversationnelle et, enfin, le déploiement d'un prototype fonctionnel."
      ],
      "metadata": {
        "id": "boZYVTSPMbb8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Activation du GPU\n",
        "Pour commencer, il faut activer le GPU afin de faciliter l'exécution du projet.\n",
        "1.   Allez dans le menu Exécution -> Modifier le type d'exécution.\n",
        "2.   Dans le menu déroulant \"Accélérateur matériel\", sélectionnez GPU T4.\n",
        "3.   Cliquez sur \"Enregistrer\".\n",
        "\n"
      ],
      "metadata": {
        "id": "02JX67cZtVAQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Créer et Téléverser vos Documents"
      ],
      "metadata": {
        "id": "5iox7ThY26Fg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzYPkVsgST1U",
        "outputId": "bd45077b-4324-4d0d-acb4-bf33197579a4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Installation d'Ollama dans le Notebook"
      ],
      "metadata": {
        "id": "ssESX4zpuvVh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24PRwOeOqSZ7",
        "outputId": "a8e6b190-d2d4-456f-fa5b-b6fb47a6a634"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ],
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Lancement d'Ollama en Arrière-Plan\n",
        "Ollama doit tourner comme un service. Cette commande le lance en arrière-plan pour que nous puissions l'utiliser dans les cellules suivantes."
      ],
      "metadata": {
        "id": "k7rCnBQmvIX4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import asyncio\n",
        "# Commande pour lancer le serveur en arrière-plan\n",
        "command = \"nohup ollama serve > ollama.log 2>&1 &\"\n",
        "# Exécuter la commande\n",
        "os.system(command)\n",
        "async def wait_for_server():\n",
        "    await asyncio.sleep(5)\n",
        "\n",
        "print(\"Serveur Ollama démarré en arrière-plan.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lbds3Ig_vbAo",
        "outputId": "44232eb2-0c02-418f-91e7-63aa1b18c7c3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Serveur Ollama démarré en arrière-plan.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Installation des Librairies Python\n",
        "Nous installons les librairies Python nécessaires ainsi que toutes leurs dépendances. Ces librairies faciliteront l'installation du modèle LLM plus tard."
      ],
      "metadata": {
        "id": "Ak81VFXD9cVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain_community chromadb sentence-transformers msoffcrypto-tool unstructured \"unstructured[pdf]\" \"unstructured[docx]\" \"unstructured[xlsx]\" \"unstructured[pptx]\" --upgrade chainlit pyngrok -U langchain-ollama langchain-huggingface"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7HRj4YNI9o1i",
        "outputId": "be7fd2ed-f309-46e8-9c63-8f3e39afcf9e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.12/dist-packages (0.3.29)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.12/dist-packages (1.0.20)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.0)\n",
            "Requirement already satisfied: msoffcrypto-tool in /usr/local/lib/python3.12/dist-packages (5.4.2)\n",
            "Requirement already satisfied: unstructured in /usr/local/lib/python3.12/dist-packages (0.18.14)\n",
            "Requirement already satisfied: chainlit in /usr/local/lib/python3.12/dist-packages (2.7.2)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.12/dist-packages (7.3.0)\n",
            "Requirement already satisfied: langchain-ollama in /usr/local/lib/python3.12/dist-packages (0.3.7)\n",
            "Requirement already satisfied: langchain-huggingface in /usr/local/lib/python3.12/dist-packages (0.3.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.75)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.9)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.16)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.10.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\n",
            "Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.4.2)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\n",
            "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.25.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.15.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.22.1)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.36.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.21.4)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.74.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.3.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.16.1)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (33.1.0)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.2.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.2)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.55.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.34.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: cryptography>=39.0 in /usr/local/lib/python3.12/dist-packages (from msoffcrypto-tool) (43.0.3)\n",
            "Requirement already satisfied: olefile>=0.46 in /usr/local/lib/python3.12/dist-packages (from msoffcrypto-tool) (0.47)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from unstructured) (3.4.3)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.12/dist-packages (from unstructured) (1.2.0)\n",
            "Requirement already satisfied: python-magic in /usr/local/lib/python3.12/dist-packages (from unstructured) (0.4.27)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from unstructured) (5.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from unstructured) (3.9.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from unstructured) (4.13.5)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.12/dist-packages (from unstructured) (2.14.1)\n",
            "Requirement already satisfied: python-iso639 in /usr/local/lib/python3.12/dist-packages (from unstructured) (2025.2.18)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.12/dist-packages (from unstructured) (1.0.9)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.12/dist-packages (from unstructured) (3.14.0)\n",
            "Requirement already satisfied: backoff in /usr/local/lib/python3.12/dist-packages (from unstructured) (2.2.1)\n",
            "Requirement already satisfied: unstructured-client in /usr/local/lib/python3.12/dist-packages (from unstructured) (0.42.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from unstructured) (1.17.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from unstructured) (5.9.5)\n",
            "Requirement already satisfied: python-oxmsg in /usr/local/lib/python3.12/dist-packages (from unstructured) (0.0.2)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.12/dist-packages (from unstructured) (1.1)\n",
            "Requirement already satisfied: onnx>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from unstructured[pdf]) (1.19.0)\n",
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.12/dist-packages (from unstructured[pdf]) (1.17.0)\n",
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.12/dist-packages (from unstructured[pdf]) (20250506)\n",
            "Requirement already satisfied: pikepdf in /usr/local/lib/python3.12/dist-packages (from unstructured[pdf]) (9.10.2)\n",
            "Requirement already satisfied: pi-heif in /usr/local/lib/python3.12/dist-packages (from unstructured[pdf]) (1.1.0)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (from unstructured[pdf]) (6.0.0)\n",
            "Requirement already satisfied: google-cloud-vision in /usr/local/lib/python3.12/dist-packages (from unstructured[pdf]) (3.10.2)\n",
            "Requirement already satisfied: effdet in /usr/local/lib/python3.12/dist-packages (from unstructured[pdf]) (0.4.1)\n",
            "Requirement already satisfied: unstructured-inference>=1.0.5 in /usr/local/lib/python3.12/dist-packages (from unstructured[pdf]) (1.0.5)\n",
            "Requirement already satisfied: unstructured.pytesseract>=0.3.12 in /usr/local/lib/python3.12/dist-packages (from unstructured[pdf]) (0.3.15)\n",
            "Requirement already satisfied: python-docx>=1.1.2 in /usr/local/lib/python3.12/dist-packages (from unstructured[docx]) (1.2.0)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (from unstructured[xlsx]) (3.1.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from unstructured[xlsx]) (2.2.2)\n",
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.12/dist-packages (from unstructured[xlsx]) (2.0.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from unstructured[xlsx]) (3.5)\n",
            "Requirement already satisfied: python-pptx>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from unstructured[pptx]) (1.0.2)\n",
            "Requirement already satisfied: aiofiles<25.0.0,>=23.1.0 in /usr/local/lib/python3.12/dist-packages (from chainlit) (24.1.0)\n",
            "Requirement already satisfied: asyncer<0.1.0,>=0.0.8 in /usr/local/lib/python3.12/dist-packages (from chainlit) (0.0.8)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.12/dist-packages (from chainlit) (8.2.1)\n",
            "Requirement already satisfied: fastapi<0.117,>=0.116.1 in /usr/local/lib/python3.12/dist-packages (from chainlit) (0.116.1)\n",
            "Requirement already satisfied: lazify<0.5.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from chainlit) (0.4.0)\n",
            "Requirement already satisfied: literalai==0.1.201 in /usr/local/lib/python3.12/dist-packages (from chainlit) (0.1.201)\n",
            "Requirement already satisfied: mcp<2.0.0,>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from chainlit) (1.13.1)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from chainlit) (1.6.0)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.12/dist-packages (from chainlit) (25.0)\n",
            "Requirement already satisfied: pyjwt<3.0.0,>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from chainlit) (2.10.1)\n",
            "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from chainlit) (1.1.1)\n",
            "Requirement already satisfied: python-multipart<1.0.0,>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from chainlit) (0.0.20)\n",
            "Requirement already satisfied: python-socketio<6.0.0,>=5.11.0 in /usr/local/lib/python3.12/dist-packages (from chainlit) (5.13.0)\n",
            "Requirement already satisfied: starlette>=0.47.2 in /usr/local/lib/python3.12/dist-packages (from chainlit) (0.47.3)\n",
            "Requirement already satisfied: syncer<3.0.0,>=2.0.3 in /usr/local/lib/python3.12/dist-packages (from chainlit) (2.0.3)\n",
            "Requirement already satisfied: tomli<3.0.0,>=2.0.1 in /usr/local/lib/python3.12/dist-packages (from chainlit) (2.2.1)\n",
            "Requirement already satisfied: watchfiles<1.0.0,>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from chainlit) (0.24.0)\n",
            "Requirement already satisfied: chevron>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from literalai==0.1.201->chainlit) (0.14.0)\n",
            "Requirement already satisfied: traceloop-sdk>=0.33.12 in /usr/local/lib/python3.12/dist-packages (from literalai==0.1.201->chainlit) (0.46.1)\n",
            "Requirement already satisfied: ollama<1.0.0,>=0.5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-ollama) (0.5.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.4.0 in /usr/local/lib/python3.12/dist-packages (from asyncer<0.1.0,>=0.0.8->chainlit) (4.10.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=39.0->msoffcrypto-tool) (1.17.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.6.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.6.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.8)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.27.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.5.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.24.0)\n",
            "Requirement already satisfied: sse-starlette>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from mcp<2.0.0,>=1.11.0->chainlit) (3.0.2)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx>=1.17.0->unstructured[pdf]) (5.29.5)\n",
            "Requirement already satisfied: ml_dtypes in /usr/local/lib/python3.12/dist-packages (from onnx>=1.17.0->unstructured[pdf]) (0.5.3)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.36.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.36.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.57b0)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.6)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: XlsxWriter>=0.5.7 in /usr/local/lib/python3.12/dist-packages (from python-pptx>=1.0.1->unstructured[pptx]) (3.2.5)\n",
            "Requirement already satisfied: bidict>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from python-socketio<6.0.0,>=5.11.0->chainlit) (0.23.1)\n",
            "Requirement already satisfied: python-engineio>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from python-socketio<6.0.0,>=5.11.0->chainlit) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: opencv-python!=4.7.0.68 in /usr/local/lib/python3.12/dist-packages (from unstructured-inference>=1.0.5->unstructured[pdf]) (4.12.0.88)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from unstructured-inference>=1.0.5->unstructured[pdf]) (3.10.0)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (from unstructured-inference>=1.0.5->unstructured[pdf]) (1.0.19)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (from unstructured-inference>=1.0.5->unstructured[pdf]) (1.10.1)\n",
            "Requirement already satisfied: pypdfium2 in /usr/local/lib/python3.12/dist-packages (from unstructured-inference>=1.0.5->unstructured[pdf]) (4.30.0)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->unstructured) (2.7)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from effdet->unstructured[pdf]) (0.23.0+cu126)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from effdet->unstructured[pdf]) (2.0.10)\n",
            "Requirement already satisfied: omegaconf>=2.0 in /usr/local/lib/python3.12/dist-packages (from effdet->unstructured[pdf]) (2.3.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision->unstructured[pdf]) (2.25.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-vision->unstructured[pdf]) (1.26.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from html5lib->unstructured) (0.5.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->unstructured) (1.5.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl->unstructured[xlsx]) (2.0.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->unstructured[xlsx]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->unstructured[xlsx]) (2025.2)\n",
            "Requirement already satisfied: Deprecated in /usr/local/lib/python3.12/dist-packages (from pikepdf->unstructured[pdf]) (1.2.18)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.4.0->asyncer<0.1.0,>=0.0.8->chainlit) (1.3.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=39.0->msoffcrypto-tool) (2.22)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision->unstructured[pdf]) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from omegaconf>=2.0->effdet->unstructured[pdf]) (4.9.3)\n",
            "Requirement already satisfied: simple-websocket>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from python-engineio>=4.11.0->python-socketio<6.0.0,>=5.11.0->chainlit) (1.1.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: colorama<0.5.0,>=0.4.6 in /usr/local/lib/python3.12/dist-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.4.6)\n",
            "Requirement already satisfied: cuid<0.5,>=0.4 in /usr/local/lib/python3.12/dist-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.4)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.28.0 in /usr/local/lib/python3.12/dist-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-alephalpha==0.46.1 in /usr/local/lib/python3.12/dist-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.46.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-anthropic==0.46.1 in /usr/local/lib/python3.12/dist-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.46.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-bedrock==0.46.1 in /usr/local/lib/python3.12/dist-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.46.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-chromadb==0.46.1 in /usr/local/lib/python3.12/dist-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.46.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-cohere==0.46.1 in /usr/local/lib/python3.12/dist-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.46.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-crewai==0.46.1 in /usr/local/lib/python3.12/dist-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.46.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-google-generativeai==0.46.1 in /usr/local/lib/python3.12/dist-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.46.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-groq==0.46.1 in /usr/local/lib/python3.12/dist-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.46.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-haystack==0.46.1 in /usr/local/lib/python3.12/dist-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.46.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-lancedb==0.46.1 in /usr/local/lib/python3.12/dist-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.46.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-langchain==0.46.1 in /usr/local/lib/python3.12/dist-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.46.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-llamaindex==0.46.1 in /usr/local/lib/python3.12/dist-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.46.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-logging>=0.57b0 in /usr/local/lib/python3.12/dist-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.57b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-marqo==0.46.1 in /usr/local/lib/python3.12/dist-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.46.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-mcp==0.46.1 in /usr/local/lib/python3.12/dist-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.46.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-milvus==0.46.1 in /usr/local/lib/python3.12/dist-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.46.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-mistralai==0.46.1 in /usr/local/lib/python3.12/dist-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.46.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-ollama==0.46.1 in /usr/local/lib/python3.12/dist-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.46.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-openai==0.46.1 in /usr/local/lib/python3.12/dist-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.46.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-openai-agents==0.46.1 in /usr/local/lib/python3.12/dist-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.46.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-pinecone==0.46.1 in /usr/local/lib/python3.12/dist-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.46.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-qdrant==0.46.1 in /usr/local/lib/python3.12/dist-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.46.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-redis>=0.50b0 in /usr/local/lib/python3.12/dist-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.57b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-replicate==0.46.1 in /usr/local/lib/python3.12/dist-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.46.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-requests>=0.50b0 in /usr/local/lib/python3.12/dist-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.57b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-sagemaker==0.46.1 in /usr/local/lib/python3.12/dist-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.46.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-sqlalchemy>=0.50b0 in /usr/local/lib/python3.12/dist-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.57b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-threading>=0.50b0 in /usr/local/lib/python3.12/dist-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.57b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-together==0.46.1 in /usr/local/lib/python3.12/dist-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.46.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-transformers==0.46.1 in /usr/local/lib/python3.12/dist-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.46.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-urllib3>=0.50b0 in /usr/local/lib/python3.12/dist-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.57b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-vertexai==0.46.1 in /usr/local/lib/python3.12/dist-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.46.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-watsonx==0.46.1 in /usr/local/lib/python3.12/dist-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.46.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-weaviate==0.46.1 in /usr/local/lib/python3.12/dist-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.46.1)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions-ai<0.5.0,>=0.4.13 in /usr/local/lib/python3.12/dist-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.4.13)\n",
            "Requirement already satisfied: opentelemetry-instrumentation>=0.50b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-instrumentation-alephalpha==0.46.1->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.57b0)\n",
            "Requirement already satisfied: anthropic>=0.17.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-instrumentation-bedrock==0.46.1->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.64.0)\n",
            "Requirement already satisfied: inflection<0.6.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-instrumentation-llamaindex==0.46.1->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.5.1)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp<2.0.0,>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-instrumentation-mcp==0.46.1->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (1.36.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->unstructured-inference>=1.0.5->unstructured[pdf]) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->unstructured-inference>=1.0.5->unstructured[pdf]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->unstructured-inference>=1.0.5->unstructured[pdf]) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->unstructured-inference>=1.0.5->unstructured[pdf]) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->unstructured-inference>=1.0.5->unstructured[pdf]) (3.2.3)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.57b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-instrumentation-requests>=0.50b0->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.57b0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Requirement already satisfied: wsproto in /usr/local/lib/python3.12/dist-packages (from simple-websocket>=0.10.0->python-engineio>=4.11.0->python-socketio<6.0.0,>=5.11.0->chainlit) (1.2.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from anthropic>=0.17.0->opentelemetry-instrumentation-bedrock==0.46.1->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.10.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Téléchargement du Modèle LLM\n",
        "Ici nous téléchargeons le modèle LLM. Nous choisissons mistral pour sa puissance et son rapport performance/taille."
      ],
      "metadata": {
        "id": "aAndckd4-4OK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull mistral"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nru6fWi89pYJ",
        "outputId": "b3c72f3f-806f-4d21-fbbc-a6067344ad8d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Configuration du ChatBot"
      ],
      "metadata": {
        "id": "y189r9dzPHwc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1. Importations de librairies"
      ],
      "metadata": {
        "id": "dihSufKyQTx0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_ollama import ChatOllama\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser"
      ],
      "metadata": {
        "id": "punNb8Kr9pci"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2. Configuration des Modèles"
      ],
      "metadata": {
        "id": "DJ5IJ3EBQ8Ec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_MODEL_NAME = \"BAAI/bge-m3\"\n",
        "OLLAMA_MODEL_NAME = \"mistral\"\n",
        "\n",
        "llm = ChatOllama(\n",
        "    model=OLLAMA_MODEL_NAME, # ex: \"mistral\" ou \"llama3.1:8b\"\n",
        "    temperature=0.2,         # Très basse pour des réponses factuelles\n",
        "    top_p=0.8,\n",
        "    top_k=50                 # On peut réduire top_k pour être plus strict\n",
        ")\n",
        "\n",
        "print(f\"Modèle LLM '{OLLAMA_MODEL_NAME}' chargé avec une température de {llm.temperature}.\")\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=EMBEDDING_MODEL_NAME,\n",
        "    model_kwargs={'device': 'cuda'},        # Force l'utilisation du GPU\n",
        "    encode_kwargs={'normalize_embeddings': True} # Active la normalisation\n",
        ")"
      ],
      "metadata": {
        "id": "38kEyEpa9pmS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b03b4e4-5866-4650-c4c9-f4068bc3112f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modèle LLM 'mistral' chargé avec une température de 0.2.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.3. Chargement et Traitement des Documents\n",
        "Nous utilisons un chargeur qui prend tous les fichiers d'un dossier. Il reconnaît automatiquement les types de fichiers (PDF, TXT, etc.).\n",
        "\n",
        "Les documents chargés sont divisés en plus petits morceaux (chunks). Cette étape est cruciale pour la performance du RAG."
      ],
      "metadata": {
        "id": "S0TdtW6ERaJE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3456d2fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc351914-e91f-4bed-93b2-6dd19010bd94"
      },
      "source": [
        "!apt-get install poppler-utils -y\n",
        "!apt-get update && apt-get install tesseract-ocr-fra -y\n",
        "!apt-get install -y ocrmypdf"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "poppler-utils is already the newest version (22.02.0-2ubuntu0.10).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n",
            "Hit:1 https://cli.github.com/packages stable InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr-fra is already the newest version (1:4.00~git30-7274cfa-1.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ocrmypdf is already the newest version (13.4.0+dfsg-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "KNOWLEDGE_BASE_DIR = \"/content/drive/Othercomputers/Mon Laptop Asus/Data_Science/Projet_Fin_Etude/knowledge_base\"\n",
        "\n",
        "print(f\"Chargement des documents depuis le dossier : {KNOWLEDGE_BASE_DIR}\")\n",
        "\n",
        "# chargeur\n",
        "loader = DirectoryLoader(KNOWLEDGE_BASE_DIR, glob=\"**/*.*\", show_progress=True, loader_kwargs={\"languages\": [\"fra\"]})\n",
        "documents = loader.load()\n",
        "\n",
        "# Divise les documents chargés en plus petits morceaux (chunks)\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(documents)\n",
        "\n",
        "print(f\"{len(documents)} document(s) chargé(s) et divisé(s) en {len(splits)} morceaux.\")"
      ],
      "metadata": {
        "id": "kJNRpIjWRamc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef46e22a-95f0-419a-aa8f-052b7c9835f0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chargement des documents depuis le dossier : /content/drive/Othercomputers/Mon Laptop Asus/Data_Science/Projet_Fin_Etude/knowledge_base\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [03:10<00:00, 31.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6 document(s) chargé(s) et divisé(s) en 109 morceaux.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Création de la base de données vectorielle\n",
        "La **base vectorielle** est essentielle pour relier un **LLM** aux documents internes d’une organisation.  \n",
        "Elle transforme ces documents en **embeddings** (vecteurs) permettant une **recherche sémantique** efficace.  \n",
        "\n",
        "Son rôle principal est d’alimenter le chatbot dans une architecture **RAG (Retrieval-Augmented Generation)** :  \n",
        "1. Indexer et vectoriser les documents internes.  \n",
        "2. Stocker les vecteurs dans une base spécialisée (Pinecone, FAISS, Weaviate, etc.).  \n",
        "3. Lors d’une question, retrouver les passages pertinents.  \n",
        "4. Fournir ces passages au LLM pour générer une réponse adaptée.  \n",
        "\n",
        "**Avantages clés :**  \n",
        "- Accès rapide aux données internes (catalogue, fiches de produits, FAQ).  \n",
        "- Recherche basée sur le sens et non sur les mots-clés.  \n",
        "- Mise à jour facile du savoir du chatbot.  \n",
        "- Réduction des erreurs et hallucinations du modèle.  \n",
        "- Contrôle et personnalisation du contenu des réponses.  \n",
        "\n",
        "La base vectorielle transforme un LLM générique en **chatbot intelligent, spécialisé et fiable**, capable de répondre avec précision à partir de vos propres documents."
      ],
      "metadata": {
        "id": "dHotG-dbRbHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Création de la Base de Données Vectorielle (à partir des documents)\n",
        "vectorstore = Chroma.from_documents(documents=splits, embedding=embedding_model)\n",
        "retriever = vectorstore.as_retriever()\n",
        "print(\"Base de connaissance vectorisée et prête.\")\n",
        "\n",
        "\n",
        "# --- Chaîne de Traitement RAG ---\n",
        "system_prompt_template = \"\"\"\n",
        "Tu es un assistant virtuel expert du service client de notre entreprise.\n",
        "Ton ton doit être professionnel, clair et serviable.\n",
        "Réponds à la question de l'utilisateur en te basant exclusivement sur le contexte suivant fourni.\n",
        "Si l'information n'est pas dans le contexte, réponds poliment que tu ne disposes pas de cette information.\n",
        "\n",
        "Contexte fourni :\n",
        "{context}\n",
        "\n",
        "Question de l'utilisateur :\n",
        "{question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(system_prompt_template)\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(\"Chaîne RAG prête !\")"
      ],
      "metadata": {
        "id": "Qtr_I0UXRbbE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f46a1a62-7d15-481c-bb49-c37501df52b3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base de connaissance vectorisée et prête.\n",
            "Chaîne RAG prête !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Logique de Conversation et de Qualification"
      ],
      "metadata": {
        "id": "ajFvLi1zRcAD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.1. Création du Gestionnaire de Conversation\n",
        "\n",
        "Cette étape est cruciale dans le projet car elle permet de structurer le dialogue entre l’utilisateur et le chatbot.  \n",
        "Le **gestionnaire de conversation** ne se limite pas à répondre aux questions : il **collecte d’abord les informations clés** (nom, secteur d’activité, lieu d’exercice, contact), ce qui permet de :\n",
        "\n",
        "1. **Qualifier l’utilisateur** : avant de fournir une réponse, le chatbot s’assure d’avoir les données minimales nécessaires pour personnaliser l’échange.  \n",
        "2. **Améliorer la pertinence des réponses** : en disposant du contexte utilisateur, le chatbot peut adapter son langage et ses conseils.  \n",
        "3. **Faciliter la traçabilité** : les informations collectées peuvent être stockées ou utilisées pour des analyses ultérieures (CRM, support client, suivi des demandes).  \n",
        "4. **Assurer une expérience utilisateur fluide** : la progression est guidée par des questions ciblées, évitant que l’utilisateur ait à tout fournir d’un seul coup.  \n",
        "5. **Optimiser l’usage du RAG** : une fois la qualification terminée, la question de fond est transmise à la chaîne RAG pour générer une réponse enrichie et documentée.  \n",
        "\n",
        "Cette section agit comme un **chef d’orchestre** du projet : elle garantit que le chatbot est capable de **comprendre qui interagit avec lui, dans quel contexte, et avec quelle question**, avant de mobiliser les documents vectorisés pour produire une réponse fiable et personnalisée."
      ],
      "metadata": {
        "id": "QviZfz4na7ky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConversationManager:\n",
        "    def __init__(self, llm, rag_chain):\n",
        "        self.llm = llm\n",
        "        self.rag_chain = rag_chain\n",
        "        # C'est ici que nous définissons les informations que nous voulons collecter.\n",
        "        self.required_info = {\n",
        "            \"nom\": None,\n",
        "            \"secteur_activité\": None,\n",
        "            \"lieu_exercice\": None,\n",
        "            \"contact\": None,\n",
        "        }\n",
        "        self.user_question = None\n",
        "        self.qualification_prompt = ChatPromptTemplate.from_template(\n",
        "            \"\"\"Extrais les informations suivantes du texte de l'utilisateur : nom, secteur_activité, lieu_exercice, contact, et la question de fond qu'il pose.\n",
        "            Réponds UNIQUEMENT avec les informations trouvées, une par ligne, comme ceci :\n",
        "            nom: [nom trouvé ou \"non trouvé\"]\n",
        "            secteur_activité: [secteur trouvé ou \"non trouvé\"]\n",
        "            lieu_exercice: [lieu trouvé ou \"non trouvé\"]\n",
        "            contact: [contact trouvé ou \"non trouvé\"]\n",
        "            question: [question de fond trouvée ou \"non trouvé\"]\n",
        "\n",
        "            Texte de l'utilisateur : \"{user_input}\"\n",
        "            \"\"\"\n",
        "        )\n",
        "        self.qualification_chain = self.qualification_prompt | self.llm | StrOutputParser()\n",
        "\n",
        "    def is_qualified(self):\n",
        "        \"\"\"Vérifie si toutes les informations requises ont été collectées.\"\"\"\n",
        "        return all(value is not None for value in self.required_info.values())\n",
        "\n",
        "    def get_next_question(self):\n",
        "        \"\"\"Détermine la prochaine question à poser.\"\"\"\n",
        "        # Personnalisez les questions posées par le chatbot.\n",
        "        if self.required_info[\"nom\"] is None:\n",
        "            return \"Bonjour ! Pour mieux vous assister, pourriez-vous commencer par vous présenter (votre nom et prénom(s)) ?\"\n",
        "        if self.required_info[\"secteur_activité\"] is None:\n",
        "            return \"Merci. Dans quel secteur d'activité travaillez-vous ?\"\n",
        "        if self.required_info[\"lieu_exercice\"] is None:\n",
        "            return \"Parfait. Où est situé le lieu d'exercice de votre activité (ville/région) ?\"\n",
        "        if self.required_info[\"contact\"] is None:\n",
        "            return \"Nous y sommes presque. Quel est votre contact (email ou numéro de téléphone) ?\"\n",
        "        return None\n",
        "\n",
        "    def process_message(self, user_input):\n",
        "        \"\"\"Traite le message de l'utilisateur et retourne la réponse du chatbot.\"\"\"\n",
        "        # Si la qualification n'est pas terminée\n",
        "        if not self.is_qualified():\n",
        "            # On utilise le LLM pour extraire les informations de la réponse de l'utilisateur\n",
        "            extracted_text = self.qualification_chain.invoke({\"user_input\": user_input})\n",
        "\n",
        "            # Mise à jour des informations collectées\n",
        "            for line in extracted_text.split('\\n'):\n",
        "                if ':' in line:\n",
        "                    key, value = line.split(':', 1)\n",
        "                    key = key.strip()\n",
        "                    value = value.strip()\n",
        "                    if key in self.required_info and value != \"non trouvé\":\n",
        "                        self.required_info[key] = value\n",
        "                    if key == \"question\" and value != \"non trouvé\":\n",
        "                        self.user_question = value\n",
        "\n",
        "            # Si on est maintenant qualifié ET qu'on a une question\n",
        "            if self.is_qualified():\n",
        "                print(\"--- QUALIFICATION TERMINÉE ---\")\n",
        "                print(\"Informations collectées :\", self.required_info)\n",
        "                if self.user_question:\n",
        "                    response = f\"Merci pour ces informations. Concernant votre question '{self.user_question}' :\\n\"\n",
        "                    response += self.rag_chain.invoke(self.user_question)\n",
        "                    return response\n",
        "                else:\n",
        "                    return \"Merci pour ces informations. Comment puis-je vous aider maintenant ?\"\n",
        "            else:\n",
        "                # Sinon, on pose la prochaine question\n",
        "                return self.get_next_question()\n",
        "\n",
        "        # Si la qualification est déjà terminée, on répond directement\n",
        "        else:\n",
        "            return self.rag_chain.invoke(user_input)\n",
        "\n",
        "# Initialisation du gestionnaire\n",
        "chatbot = ConversationManager(llm, rag_chain)\n",
        "print(\"Gestionnaire de conversation prêt !\")"
      ],
      "metadata": {
        "id": "QRdCBOJ5RcT7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b08912c-926c-4fec-9187-3ffac9eb1bcc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gestionnaire de conversation prêt !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.2. Simulations"
      ],
      "metadata": {
        "id": "4vkfZRidgHzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "entree_utilisateur = \"Bonjour, Qui es-tu?\"\n",
        "chatbot.process_message(entree_utilisateur)"
      ],
      "metadata": {
        "id": "3yP_YCI9j4tl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "22a1b51e-c9b8-4779-a795-92a6459287cf"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- QUALIFICATION TERMINÉE ---\n",
            "Informations collectées : {'nom': 'Non trouvé', 'secteur_activité': 'Non trouvé', 'lieu_exercice': 'Non trouvé', 'contact': 'Non trouvé'}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Merci pour ces informations. Concernant votre question 'Qui es-tu?' :\\n Je suis un assistant virtuel expert du service client de notre entreprise. Mon rôle est d'aider les clients à trouver des informations et des solutions en utilisant le contexte fourni dans le cadre de notre projet actuel.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "entree_utilisateur = \"Bonjour\"\n",
        "\n",
        "reponse_chatbot = chatbot.process_message(entree_utilisateur)\n",
        "print(f\"🤖 Chatbot: {reponse_chatbot}\\n\")"
      ],
      "metadata": {
        "id": "jXT3wyM3oQ4V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0f4a601-30d2-4b5b-bee6-09e5c68068d2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🤖 Chatbot:  Bonjour !\n",
            "\n",
            "Nous avons des projets d'installation de points de correspondance et les moyens de paiement via notre application mobile APV2, ainsi que l'ouverture d'autres agences. Nos produits de crédit sont conçus pour être simples, faciles et adaptés aux besoins du client. Nous avons également des produits d'épargne pour garantir à nos clients un avenir meilleur.\n",
            "\n",
            "Pour plus d'informations sur nos services, je vous invite à consulter notre catalogue de produits 2025 en ligne. Si vous avez des questions spécifiques ou si vous souhaitez discuter de votre situation particulière, n'hésitez pas à me contacter.\n",
            "\n",
            "Nous sommes au service de nos partenaires avec dynamisme et efficacité, et nous tenons nos promesses. Nous savons créer la confiance, récompenser et motiver nos équipes, être juste, équitable, transparent et accueillir les décisions difficiles avec sagesse.\n",
            "\n",
            "Je suis à votre disposition pour vous aider.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Création de l'application"
      ],
      "metadata": {
        "id": "8n9jv24M2x9v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.1. Ecriture du script complet de l'application"
      ],
      "metadata": {
        "id": "RpuI6S5abbIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import os\n",
        "import chainlit as cl\n",
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_ollama import ChatOllama\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Configuration des Modèles\n",
        "EMBEDDING_MODEL_NAME = \"BAAI/bge-m3\"\n",
        "OLLAMA_MODEL_NAME = \"mistral\"\n",
        "\n",
        "llm = ChatOllama(\n",
        "    model=OLLAMA_MODEL_NAME,\n",
        "    temperature=0.2,\n",
        "    top_p=0.8,\n",
        "    top_k=50\n",
        ")\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=EMBEDDING_MODEL_NAME,\n",
        "    model_kwargs={'device': 'cuda'},\n",
        "    encode_kwargs={'normalize_embeddings': True}\n",
        ")\n",
        "\n",
        "# Chargement et Préparation de la Base de Connaissance\n",
        "KNOWLEDGE_BASE_DIR = \"/content/drive/Othercomputers/Mon Laptop Asus/Data_Science/Projet_Fin_Etude/knowledge_base\"\n",
        "\n",
        "loader = DirectoryLoader(KNOWLEDGE_BASE_DIR, glob=\"**/*.*\", show_progress=True)\n",
        "documents = loader.load()\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(documents)\n",
        "vectorstore = Chroma.from_documents(documents=splits, embedding=embedding_model)\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# Création de la Chaîne RAG\n",
        "system_prompt_template = \"\"\"\n",
        "Tu es un assistant virtuel expert du service client de Baobab Burkina.\n",
        "Ton ton doit être professionnel, clair et serviable.\n",
        "Réponds à la question de l'utilisateur en te basant exclusivement sur le contexte suivant fourni.\n",
        "Si l'information n'est pas dans le contexte, réponds poliment que tu ne disposes pas de cette information.\n",
        "\n",
        "Contexte fourni :\n",
        "{context}\n",
        "\n",
        "Question de l'utilisateur :\n",
        "{question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(system_prompt_template)\n",
        "rag_chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Logique de Conversation\n",
        "class ConversationManager:\n",
        "    def __init__(self, llm, rag_chain):\n",
        "        self.llm = llm\n",
        "        self.rag_chain = rag_chain\n",
        "        self.required_info = {\n",
        "            \"nom\": None,\n",
        "            \"secteur_activité\": None,\n",
        "            \"lieu_exercice\": None,\n",
        "            \"contact\": None,\n",
        "        }\n",
        "        self.user_question = None\n",
        "        self.qualification_prompt = ChatPromptTemplate.from_template(\n",
        "            \"\"\"Extrais les informations suivantes du texte de l'utilisateur : nom, secteur_activité, lieu_exercice, contact, et la question de fond qu'il pose.\n",
        "            Réponds UNIQUEMENT avec les informations trouvées, une par ligne, comme ceci :\n",
        "            nom: [nom trouvé ou \"non trouvé\"]\n",
        "            secteur_activité: [secteur trouvé ou \"non trouvé\"]\n",
        "            lieu_exercice: [lieu trouvé ou \"non trouvé\"]\n",
        "            contact: [contact trouvé ou \"non trouvé\"]\n",
        "            question: [question de fond trouvée ou \"non trouvé\"]\n",
        "\n",
        "            Texte de l'utilisateur : \"{user_input}\"\n",
        "            \"\"\"\n",
        "        )\n",
        "        self.qualification_chain = self.qualification_prompt | self.llm | StrOutputParser()\n",
        "\n",
        "    def is_qualified(self):\n",
        "        return all(value is not None for value in self.required_info.values())\n",
        "\n",
        "    def get_next_question(self):\n",
        "        if self.required_info[\"nom\"] is None:\n",
        "            return \"Pour mieux vous assister, pourriez-vous commencer par vous présenter (votre nom et prénom(s)) ?\"\n",
        "        if self.required_info[\"secteur_activité\"] is None:\n",
        "            return \"Merci. Dans quel secteur d'activité travaillez-vous ?\"\n",
        "        if self.required_info[\"lieu_exercice\"] is None:\n",
        "            return \"Parfait. Où est situé le lieu d'exercice de votre activité (ville/région) ?\"\n",
        "        if self.required_info[\"contact\"] is None:\n",
        "            return \"Nous y sommes presque. Quel est votre contact (email ou numéro de téléphone) ?\"\n",
        "        return None\n",
        "\n",
        "    def process_message(self, user_input):\n",
        "        if not self.is_qualified():\n",
        "            extracted_text = self.qualification_chain.invoke({\"user_input\": user_input})\n",
        "            for line in extracted_text.split('\\n'):\n",
        "                if ':' in line:\n",
        "                    key, value = line.split(':', 1)\n",
        "                    key = key.strip()\n",
        "                    value = value.strip()\n",
        "                    if key in self.required_info and value.lower() != \"non trouvé\":\n",
        "                        self.required_info[key] = value\n",
        "                    if key == \"question\" and value.lower() != \"non trouvé\" and not self.user_question:\n",
        "                        self.user_question = value\n",
        "            if self.is_qualified():\n",
        "                if self.user_question:\n",
        "                    response = f\"Merci pour toutes ces informations. Concernant votre question '{self.user_question}' :\\n\"\n",
        "                    response += self.rag_chain.invoke(self.user_question)\n",
        "                    return response\n",
        "                else:\n",
        "                    return \"Merci pour ces informations. Comment puis-je vous aider maintenant ?\"\n",
        "            else:\n",
        "                return self.get_next_question()\n",
        "        else:\n",
        "            return self.rag_chain.invoke(user_input)\n",
        "\n",
        "# Logique d'Interface avec Chainlit\n",
        "@cl.on_chat_start\n",
        "async def start_chat():\n",
        "    \"\"\"\n",
        "    Cette fonction est appelée au début de chaque nouvelle conversation.\n",
        "    \"\"\"\n",
        "    # On initialise le gestionnaire de conversation et on le stocke dans la session\n",
        "    chatbot = ConversationManager(llm, rag_chain)\n",
        "    cl.user_session.set(\"chatbot\", chatbot)\n",
        "\n",
        "    # On envoie un message de bienvenue général à l'utilisateur.\n",
        "    await cl.Message(\n",
        "        content=\"Bonjour, \\nJe suis Yaaba Bot, l'assistant virtuel de Baobab Burkina. \\nComment puis-je vous aider aujourd'hui ?\"\n",
        "    ).send()\n",
        "\n",
        "@cl.on_message\n",
        "async def main(message: cl.Message):\n",
        "    \"\"\"\n",
        "    Cette fonction est appelée à chaque fois que l'utilisateur envoie un message.\n",
        "    \"\"\"\n",
        "    # Toute la logique passe par ici.\n",
        "\n",
        "    # On récupère le gestionnaire de conversation de la session\n",
        "    chatbot = cl.user_session.get(\"chatbot\")\n",
        "\n",
        "    # On traite le message de l'utilisateur avec notre logique complète\n",
        "    # La méthode process_message va gérer à la fois la qualification et la réponse.\n",
        "    response = chatbot.process_message(message.content)\n",
        "\n",
        "    # On envoie la réponse du chatbot\n",
        "    await cl.Message(content=response).send()"
      ],
      "metadata": {
        "id": "NCsEqR0B2y4G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f59b7026-f23d-4e90-812d-33f7fc7239a4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.2. Configuration de l'interface Chainlit"
      ],
      "metadata": {
        "id": "xcQa1ZxdpsID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!mkdir -p .chainlit"
      ],
      "metadata": {
        "id": "L3stSrh_psmK"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%%writefile .chainlit/config.toml\n",
        "# [project]\n",
        "# public = true\n",
        "# database = \"local\"   # ou \"cloud\"\n",
        "# enable_telemetry = true\n",
        "# session_timeout = 3600\n",
        "\n",
        "# [ui]\n",
        "# Nom affiché en haut de l’interface\n",
        "# name = \"Assistant Baobab\"\n",
        "\n",
        "# Logo (optionnel)\n",
        "# logo = \"https://baobab.com/wp-content/uploads/2023/08/Logo.svg\"\n",
        "\n",
        "# Thème (⚠️ l’option default_theme est dépréciée → maintenant : \"theme\")\n",
        "#theme = \"light\"   # valeurs possibles : \"light\" ou \"dark\""
      ],
      "metadata": {
        "id": "zuHaMHjfpvO0"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. Lancement de la démo"
      ],
      "metadata": {
        "id": "eZXiKkTC2zZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Obtenir l'authtoken ngrok à partir de Colab secrets\n",
        "NGROK_AUTH_TOKEN = userdata.get('NGROK_AUTH_TOKEN')\n",
        "\n",
        "# Authentifier ngrok\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "# Lancer le tunnel ngrok sur le port 8000 (port par défaut de Chainlit)\n",
        "public_url = ngrok.connect(8000)\n",
        "print(f\"La démo est accessible publiquement à cette adresse : {public_url}\")\n",
        "\n",
        "# Lancer l'application Chainlit en arrière-plan\n",
        "# Le \"-h\" est pour \"headless\", ce qui évite d'ouvrir une interface dans Colab\n",
        "os.system(\"chainlit run app.py -h &\")"
      ],
      "metadata": {
        "id": "hFkA2fKj2zjm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afd43233-4400-45dd-9868-a8b1745396f5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La démo est accessible publiquement à cette adresse : NgrokTunnel: \"https://ca6015ac8b7e.ngrok-free.app\" -> \"http://localhost:8000\"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Version de débogage pour voir les logs\n",
        "#!chainlit run app.py"
      ],
      "metadata": {
        "id": "ihTKFGpaFbJw"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Supprime le fichier de configuration incompatible\n",
        "#!rm /content/.chainlit/config.toml"
      ],
      "metadata": {
        "id": "Ha3EZH_Yei2o"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}